{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branch_length\n",
    "import distributions\n",
    "import optimizers\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "alt.renderers.enable(\"notebook\")\n",
    "\n",
    "import importlib \n",
    "importlib.reload(branch_length)\n",
    "importlib.reload(distributions)\n",
    "importlib.reload(optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytest import approx\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating gradient calculation\n",
    "\n",
    "In this section we validate the multi-sample reparametrization gradient of log ratio of p/q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_loc_val = -2.\n",
    "true_shape_val = 1.4\n",
    "true_normal = tfp.distributions.Normal(loc=true_loc_val, scale=true_shape_val)\n",
    "std_normal = tfp.distributions.Normal(loc=0., scale=1.)\n",
    "\n",
    "epsilon = tf.constant([-0.1, 0.14, -0.51, -2., 1.8])\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    mu = tf.constant(1.1)\n",
    "    sigma = tf.constant(1.)\n",
    "    g.watch(mu)\n",
    "    g.watch(sigma)\n",
    "\n",
    "    tf_x = mu + sigma * epsilon\n",
    "    y = tf.math.log(\n",
    "        # In principle we should have a 1/K term here, but it disappears in the log grad.\n",
    "        tf.math.reduce_sum(true_normal.prob(tf_x) / \n",
    "                           tfp.distributions.Normal(loc=mu, scale=sigma).prob(tf_x)))\n",
    "\n",
    "x = np.array([tf_x.numpy()]).transpose()\n",
    "tf_gradient = [grad.numpy() for grad in g.gradient(y, [mu, sigma])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution we wish to approximate-- we pretend this is the \"phylogenetic\" distribution.\n",
    "d = distributions.Normal(1)\n",
    "true_loc = np.array([true_loc_val])\n",
    "true_shape = np.array([true_shape_val])\n",
    "phylo_log_like = lambda x: d.log_prob(x, true_loc, true_shape)\n",
    "phylo_log_like_grad = lambda x: d.log_prob_grad(x, true_loc, true_shape)\n",
    "\n",
    "# Variational distribution\n",
    "q = distributions.Normal(1)\n",
    "loc = np.array([1.1])\n",
    "shape = np.array([1.])\n",
    "q_log_like = lambda x: q.log_prob(x, loc, shape)\n",
    "\n",
    "def complete_grad(x, loc, shape):\n",
    "    weights = branch_length.like_weights(q, phylo_log_like(x), x, loc, shape)\n",
    "    return branch_length.param_grad(q, weights, phylo_log_like_grad(x), x, loc, shape)\n",
    "\n",
    "loc_grad, shape_grad = complete_grad(x, loc, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the tensorflow gradient is equal to the hand-calculated gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf_gradient[0] == approx(loc_grad, rel=1e-5)\n",
    "assert tf_gradient[1] == approx(shape_grad, rel=1e-5)\n",
    "tf_gradient, loc_grad, shape_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running gradient ascent\n",
    "\n",
    "Here we actually run the gradient ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_length_param_count = 1\n",
    "sgd_server_args = {'loc': branch_length_param_count, 'shape': branch_length_param_count}\n",
    "step_count = 400\n",
    "stepsz = 0.02\n",
    "stepsz_dict = {'loc': stepsz, 'shape': stepsz}\n",
    "\n",
    "def gradient_step(infer_opt, sample_count):\n",
    "    global loc, shape\n",
    "    x = q.sample(loc, shape, sample_count)\n",
    "    loc_grad, shape_grad = complete_grad(x, loc, shape)\n",
    "    update_dict = infer_opt.adam(stepsz_dict, {'loc': loc, 'shape': shape}, \n",
    "                                 {'loc': loc_grad, 'shape': shape_grad})\n",
    "    loc += update_dict['loc']\n",
    "    shape += update_dict['shape']\n",
    "    return kl_div(phylo_log_like, q_log_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is some plotting code that can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.linspace(-5, 5, 40)\n",
    "x_vals_transpose = np.transpose(np.array([x_vals]))\n",
    "\n",
    "def plot_functions(f_true, f_approx):\n",
    "    x_transpose = np.transpose(np.array([x_vals]))\n",
    "    data = pd.DataFrame({\"x\": x_vals, \"truth\": f_true(x_vals_transpose), \n",
    "                         \"approx\": f_approx(x_vals_transpose)})\n",
    "    return alt.Chart(data.melt(id_vars=[\"x\"])).mark_line().encode(\n",
    "        x='x',\n",
    "        y='value',\n",
    "        color='variable'\n",
    "    )\n",
    "\n",
    "def kl_div(f_true, f_approx):\n",
    "    return {\n",
    "        \"standard\": stats.entropy(f_true(x_vals_transpose), f_approx(x_vals_transpose)),\n",
    "        \"reversed\": stats.entropy(f_approx(x_vals_transpose), f_true(x_vals_transpose))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs the optimization and collects the results to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(step_count, particle_count):\n",
    "    global loc, shape\n",
    "    loc = np.array([0.5])\n",
    "    shape = np.array([1.])\n",
    "    results = [kl_div(phylo_log_like, q_log_like)]\n",
    "    infer_opt = optimizers.SGD_Server(sgd_server_args)\n",
    "\n",
    "    for _ in range(step_count):\n",
    "        results.append(gradient_step(infer_opt, particle_count))\n",
    "\n",
    "    plot_data = pd.DataFrame(results).reset_index()\n",
    "\n",
    "    return alt.Chart(\n",
    "        pd.melt(plot_data, id_vars=[\"index\"], var_name=\"variant\", value_name=\"KL divergence\")\n",
    "        ).mark_line().encode(\n",
    "            alt.X(\"index\"),\n",
    "            alt.Y(\"KL divergence\",\n",
    "                  scale=alt.Scale()),\n",
    "            color=\"variant\",\n",
    "            tooltip=['index', 'KL divergence']\n",
    "        ).interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that a single particle converges quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_optimization(step_count=step_count, particle_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit is quite reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_functions(phylo_log_like, q_log_like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand the fitting procedure for more particles is a lot worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_optimization(step_count=step_count, particle_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_functions(phylo_log_like, q_log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_optimization(step_count=0, particle_count=10)\n",
    "plot_functions(phylo_log_like, q_log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gradients(particle_count, sample_count):\n",
    "    def sample_gradient():\n",
    "        x = q.sample(loc, shape, 1)\n",
    "        loc_grad, shape_grad = complete_grad(x, loc, shape)\n",
    "        pcs = str(particle_count)\n",
    "        return {\"loc_grad_\"+pcs: loc_grad[0], \"shape_grad_\"+pcs: shape_grad[0]}\n",
    "    return pd.DataFrame([sample_gradient() for _ in range(sample_count)])\n",
    "\n",
    "sample_count = 5000\n",
    "raw = pd.concat([sample_gradients(1, sample_count), sample_gradients(10, sample_count)], axis=1)\n",
    "\n",
    "def compare_pair(key1, key2):\n",
    "    compare = pd.DataFrame({k: raw[k].sort_values().reset_index(drop=True) for k in [key1, key2]})\n",
    "    scale = alt.Scale(domain=[compare.values.min(), compare.values.max()])\n",
    "    return alt.Chart(compare, width=500, height=500).mark_point().encode(\n",
    "            alt.X(key1, scale=scale),\n",
    "            alt.Y(key2, scale=scale)\n",
    "    )\n",
    "compare_pair(\"loc_grad_1\", \"loc_grad_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_pair(\"shape_grad_1\", \"shape_grad_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pair2(key1, key2):\n",
    "    compare = pd.DataFrame({k: raw[k].sort_values().reset_index(drop=True) for k in [key1, key2]})\n",
    "    compare['difference']= compare[key1]-compare[key2]\n",
    "    compare['ratio']= compare[key1]/compare[key2]\n",
    "    return compare\n",
    "compare_loc = compare_pair2(\"loc_grad_1\", \"loc_grad_10\").reset_index()\n",
    "alt.Chart(compare_loc, width=500, height=500)\\\n",
    "    .mark_point(clip=True).encode(\n",
    "        alt.X(\"index\"),\n",
    "        alt.Y(\"difference\", scale=alt.Scale(domain=(-0.3, 0.3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(compare_loc, width=500, height=500)\\\n",
    "    .mark_point(clip=True).encode(\n",
    "        alt.X(\"index\"),\n",
    "        alt.Y(\"ratio\", scale=alt.Scale(domain=(0, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_shape = compare_pair2(\"shape_grad_1\", \"shape_grad_10\").reset_index()\n",
    "alt.Chart(compare_shape, width=500, height=500)\\\n",
    "    .mark_point(clip=True).encode(\n",
    "        alt.X(\"index\"),\n",
    "        alt.Y(\"difference\", scale=alt.Scale(domain=(-0.2, 0.1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(compare_shape, width=500, height=500)\\\n",
    "    .mark_point(clip=True).encode(\n",
    "        alt.X(\"index\"),\n",
    "        alt.Y(\"ratio\", scale=alt.Scale(domain=(0, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
